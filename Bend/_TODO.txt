

----[ TODO ]---------

  - write sorted-intersect extension
  

  - on restart/recover, make sure segments are added to merge manager oldest-generation-to-newest
  - figure out the best way to fix IScanner so handle all operators  ( < <= > >= ) on all endpoints
  - check that we never try to write something bigger than a microblock

  - DBG-GUI:       
	   - visualize merges as they occur (w/progress)
       - visualize log/workingsegment/checkpoint status
	   - color blocks based on has of key range (tomn)

- email indexer test -  to test "custom block handlers"
   - given a directory of mbox files, MIME parse and index email messages, provide a query interface
   - write a custom block handler for the index hits and efficient search intersection
   - compare indexing and query performance with lucene/xapian
   - Q: how does it store "large" document blobs?

- write scanForward() version of segment walk!!

  - fix runaway max-generation issue....  
     Q: Can we just ditch "max generations" and rely on MergeManager to do all generation number assignment?
     Q: How to make "new" merge all produce the same performance as "old" merge all with numgen-hack
	   - collapse max-generation number when possible?
	   - keep a list of "live generations"?

  - fix 'unmergable' higher generation blocks (i.e. increasing order insertion)
     - (a) use MergeManager to pick a the 'minGenerationForKeyrange"
	 - or, (b) allow MergeManager to recommend a 'rename' to a lower generation


  - make flushWorkingSegment an automatic part of LayerManager

  - cleanup    
    - use SegmentDescriptor class everywhere	
	- change generation to "int" everywhere.
	- make a SegmentPointer class that contains magic/address/length/checksum, to handle segment pointers
	- drop tombstones when writing into generation-0 (oldest)
	- fix "clear segment cache hack"... why does it need to be cleared?
	- scanForward/scanBackward should really have easy to use >/< operators in addition to <=/>=

	- add unit tests:
		- write a layertest for the deletion tombstone-shadow problem        
		- mergeSegments() unit test


	Q: what should be the type of keyparts? 

    Q: How do we assure a valid merge? (no generation inversion because of partial keyranges)
       - code to check that using the min-gen for a merge output is valid?


- PERFORMANCE
  - fixes:
    - row-range consistant cache (for system metadata rows)
	- efficient row-scan instead of repeatin getNext()
  - tests:
    - simple text-indexer test
    - simulate iibench 
       - insert 1B rows while maintaining three multi-column secondary indicies (i.e. insert 3B rows)
       - 1000 rows per batch, tokudb 30k-15k inserts per sec, mysql 40k-876 inserts per sec

--------[ Bigger TODO items ]-----------

*** "table management"
  - integration of my RowPIPE prototype code, which is a generic type of "table management" to organize how multi-part keys are built
  - change system metadata to use rowpipe

*** LinearAllocationRegion implementation (so two simultaneous allocators don't interleave)
  - can MergeManager trigger "defragmentation" of existing blocks? (i.e. schedule a merge of 10 fragmented blocks in the same generation)

*** row attributes for MVCC and locks
(a) introduce row-attribute concept (can affect commit, row visibility, etc.)
(b) attach MVCC attribute handler to writes, attach txn id to each row
(c) txn-id and MVCC attribute can transparently 'fall off' a row if txn commits, 

*** freelist
  - 'real' freelist handling
  - cleaner way to handle code and encoding of stored data, such as block pointers, freelist.. (binstruct?)

*** raw-byterange RegionManager
  - bounds checking on log-write, circular log, log-extension capability
  - bounds checking on segment size
  - encode segment length in SegmentInfo

*** Automated Replication
(a) 'lock' for old generations (i.e. checkpoint),
(b) copy locked data to replica
(c) create new 'lock' for newer data, goto (b), until we are up-to-date

*** try using C# sqlite sql implementation to throw SQL on top

- fix update semantics so "setValue" doesn't hack the value directly into the in-memory segment. It should simply commit the change packet and handleCommand() should cause the write to occur, just as it does in recovery. (i.e. recovery is the same as update)
- SegmentWriter needs buffered output (LogAppendWriter) - there was some problem with just dropping a buffered stream in place of the current streams
- Singleton keyparts for less allocation + faster comparisons
- stats tracking (qps, w_qps at 1 second interval)

*** I/O scheduling (so log writes and segment writes don't thrash)
*** layer-avoidance, bloom filters
*** distributed shard manager
*** xml document database
*** Network Interface

*** Interactive C# Shell mode?
   - http://tirania.org/blog/archive/2008/Sep-08.html


------------[ DONE ]--------------------
11/23 - prevent visual 'generation skipping'  (where it skips generation-columns that have no data) (tomn)
11/23 - draw blocks the same height, with diagonal lines to indiciate key range alignment (tomn)
11/23 - write scanForward for the database (stand in until we write a real scanForward sorted-walk)
11/20 - added merge manager 'key histogram' merge-ratio computation as a fallback
11/20 - first incremental merge manager (+prevent holes from 'blocking' merges)
11/18 - make a "big test case" that shows off multiple layers of segments merging, with a debug visualization UI!
11/3 - first incremental block merge works
